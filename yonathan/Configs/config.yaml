# The scheme of this object is important - don't change it, change only the values
Visibility:
  interactiveSession: True

RunningSpecs:
  distributed: False
  FlagAt: NOFLAG
  isFit: True
  processed_data: '5_extended' # The name of the model result directory
  

Datasets:
  dummyds: False

Logging:
  ENABLE_LOGGING: True
  log_fname: 'log.txt'

Folders:
  data_dir: "../data" # relative to (?)
  avatars: "avatars"
  samples: "samples"
  conf: "conf"
  results: "results"

strings:
  features_strings:
    - "Avatar"
    - "Tilt"
    - "Background type"
    - "Clothes type"
    - "Glasses type"
    - "Hair type"
    - "Mustache type"

Losses:
  use_td_loss: False
  use_bu1_loss: False
  use_bu2_loss: True
  activation_fun: 'ReLU' 
  # the name here must be the same as the name of the activation function under `torch.nn`
  # there are some extra params set up in the `Config.py` file

Models:
  features: "all"
  num_heads: "only_features"
  inshape: [3, 112, 224] # `[int,int,int]` the spahe of the incoming data. The inshape being a list rather than a tuple is simpler, since a YAML formatter would treat the tuple as a string, and accessing the elements of the tuple would just concat the string '('.
  nclasses: [[48], [48], [48], [48]] # the number of classes within the dataset
  strides: [2, 2, 1, 2]
  nfilters: [64, 96, 128, 256] # ResNet filters
  ns: [0, 2, 2, 2, 2] # the number of blocks in 
  ks: [7, 3, 3, 3, 3]
  ntaskhead_fc: 1 # get out only 1 task!
  use_lateral_tdbu: True  # ('bool') use lateral connections in the TDBU
  use_lateral_butd: True # ('bool') use lateral connections in the TDBU
  # these 2 booleans are connected to the Losses section
  use_final_conv: False


# Some other model params are initialized in the function `init_model_options` under `v26.functions.inits.py`
# TODO: change this to 2 config files - one for the frequent params and one for the `defaults.yaml` file


Training:
  bs: 32 # batch size (int)
  num_workers: 2
  epochs: 200
  momentum: 0.9 # (`float`) Momentum of the Adam optimizer
  weight_decay: 0.0001 #(`float`) The weight decay of the Adam optimizer
  lr: 2e-3 # (`float`)the initial learning rate # TODO support in also multiplication
  max_lr: 0.002  # (`float`) the maximum learning rate
  distributed: False # whether to use distributed data within training - accross some GPUs using DistributedSampler of torch
  cycle_lr: True # (`bool`) whether to cycle the learning rate
  optimizer: Adam  # (`str`) the optimizer to use. ['Adam','SGD']
  

  # checkpoints and saving options
  epoch_save_idx: accuracy # (`str`) the metric to use for saving the model
  checkpoints_per_epoch: 1 # number of checkpoints of the model weights to save per epoch
  dataset_to_save: 'val' # ['train','test','val'] the dataset to save the model weights to 
  
  
  # other options
  multiprocessing_distributed: False #Whether to use multiprocessing_distributed data
  load_model_if_exists: True # whether to load (from checkpoint) the model from an old training. Useful since some of the envs collapse during training....a

