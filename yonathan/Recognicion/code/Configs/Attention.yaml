---
# this is configuration file for attention model
hidden_size: 256 # (int, optional, defaults to 768) – Dimensionality of the encoder layers and the pooler layer.
num_hidden_layers: 4 # (int, optional, defaults to 12) – Number of hidden layers in the Transformer encoder.
num_attention_heads: 4 #(int, optional, defaults to 12) – Number of attention heads for each attention layer in the Transformer encoder.
intermediate_size: 3072 #(int, optional, defaults to 3072) – Dimensionality of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.
hidden_act: "relu" #(str or function, optional, defaults to "gelu") – The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu", "selu" and "gelu_new" are supported.
hidden_dropout_prob: 0.1 #(float, optional, defaults to 0.1) – The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
attention_probs_dropout_prob: 0.1 # (float, optional, defaults to 0.1) – The dropout ratio for the attention probabilities.
initializer_range: 0.02 #(float, optional, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
layer_norm_eps: # (float, optional, defaults to 1e-12) – The epsilon used by the layer normalization layers.
gradient_checkpointing: False # (bool, optional, defaults to False) – If True, use gradient checkpointing to save memory at the expense of slower backward pass.
image_size: 224 #(int, optional, defaults to 224) – The size (resolution) of each image.
patch_size: 16 #(int, optional, defaults to 16) – The size (resolution) of each patch.
num_channels: 3 # (int, optional, defaults to 3) – The number of input channels.
